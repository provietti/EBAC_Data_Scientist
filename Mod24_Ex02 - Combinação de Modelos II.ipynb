{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3afd16",
   "metadata": {},
   "source": [
    "# EBAC - Profissão Cientista de Dados\n",
    "## Módulo 23: Combinação de Modelos I\n",
    "## Exercício 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd6466",
   "metadata": {},
   "source": [
    "**1) Cite 5 diferenças entre o AdaBoost e o GBM**\n",
    "\n",
    "- O primeiro passo do AdaBoost é criar um stump que fará a primeira predição, já no GBM a primeira predição é uma previsão simples, geralmente a média da variável e com isso calcula-se os resíduos.\n",
    "\n",
    "- Os classificadores fracos do Adaboost são stumps enquanto no GBM são árvores\n",
    "\n",
    "- No AdaBoost cada resposta das árvores tem um peso diferente, já no GBM todas as respostas das árvores possui um multiplicador em comum chamado learning_rate\n",
    "\n",
    "- Os classificadores fracos do Adaboost vão predizer a variável dependente Y, já no GBM eles irão predizer os resíduos\n",
    "\n",
    "- No Adaboost a resposta final de uma regressão é a média dos valores preditos de cada classificador fraco poderada pelos seus pesos, já no GBM, como os  classificaodres, a resposta final dos classificadores fracos multiplicados pela taxa de aprendizado somada a previsão incial que gerealmente é a média da variável que se quer prever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181bdb0",
   "metadata": {},
   "source": [
    "**2) Acesse o link Scikit-learn – GBM, leia a explicação\n",
    "(traduza se for preciso) e crie um jupyter notebook\n",
    "contendo o exemplo de classificação e de regressão\n",
    "do GBM.**\n",
    "https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a6287",
   "metadata": {},
   "source": [
    "**Classificação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57997163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 359 ms\n",
      "Wall time: 2.43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.datasets import make_hastie_10_2 # Função para gerar dados sintéticos de classificação binária\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Implementação do classificador de Gradient Boosting do scikit-learn\n",
    "\n",
    "# Gerando um conjunto de dados sintéticos com 10 características e 2 classes (-1 e +1)\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Separando o conjunto de dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "# Criando um modelo de classificação de Gradient Boosting com hiperparâmetros específicos e treinando o classificador de Gradient Boosting nos dados de treinamento\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# Avaliando o desempenho do modelo nos dados de teste e calculando a pontuação de acurácia\n",
    "clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef66cc5",
   "metadata": {},
   "source": [
    "**Regressão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22a4f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 40.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error # Função para calcular o erro quadrático médio\n",
    "from sklearn.datasets import make_friedman1 # Função para gerar dados sintéticos do conjunto de Friedman\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Implementação do regressor de Gradient Boosting do scikit-learn\n",
    "\n",
    "# Gerando um conjunto de dados sintéticos do conjunto de Friedman com 1200 amostras e ruido adicional de 1.0\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "# Separando o conjunto de dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Criando um regressor de Gradient Boosting com hiperparâmetros específicos e\n",
    "# treinando o regressor de Gradient Boosting com os dados de treinamento\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,loss='squared_error').fit(X_train, y_train)\n",
    "\n",
    "# Calculando o erro quadrático médio (Mean Squared Error - MSE) do modelo nos dados de teste\n",
    "mean_squared_error(y_test, est.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb6a82",
   "metadata": {},
   "source": [
    "**3) Cite 5 Hyperparametros importantes no GBM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca928c",
   "metadata": {},
   "source": [
    "- **n_estimators:** O número de etapas de boosting (ou árvores) que serão ajustadas. Esse parâmetro controla o número de árvores de decisão individuais no ensemble.\n",
    "\n",
    "\n",
    "- **learning_rate:** Taxa de aprendizado (ou passo) que controla a contribuição de cada árvore no ensemble. Uma taxa de aprendizado menor significa que cada árvore contribuirá menos para o modelo final, tornando o processo de ajuste mais lento, mas geralmente resultando em um modelo mais preciso.\n",
    "\n",
    "\n",
    "- **max_depth:** A profundidade máxima das árvores de decisão individuais. Define o número máximo de níveis de divisão que cada árvore pode ter. Controlar a profundidade pode ajudar a evitar overfitting, já que árvores mais rasas tendem a ser menos complexas.\n",
    " \n",
    " \n",
    "- **min_samples_split:** O número mínimo de amostras necessárias para dividir um nó interno em uma árvore de decisão. Isso controla o tamanho mínimo dos grupos de amostras que ainda podem ser divididos. Um valor maior reduzirá a complexidade do modelo e pode evitar overfitting.\n",
    "\n",
    "\n",
    "- **loss:** A função de perda (loss function) a ser otimizada durante o ajuste do modelo. Essa função de perda mede a qualidade do ajuste do modelo em cada etapa de boosting. No scikit-learn, alguns exemplos comuns são 'ls' para regressão de mínimos quadrados (Least Squares) e 'deviance' para classificação de perda logística (Logistic Regression). Para regressão, pode-se usar 'lad' para regressão de mediana absoluta (Least Absolute Deviations).<br>O hiperparâmetro de loss pode variar de acordo com o problema específico em questão e as necessidades do modelo, e é importante escolher a função de perda adequada para o tipo de tarefa sendo resolvida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c9b51",
   "metadata": {},
   "source": [
    "**4) (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf0a24",
   "metadata": {},
   "source": [
    "**Classificação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eb5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'learning_rate': 1.0, 'max_depth': 1, 'n_estimators': 150}\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_hastie_10_2 # Função para gerar dados sintéticos de classificação binária\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Implementação do classificador de Gradient Boosting do scikit-learn\n",
    "\n",
    "# Gerando um conjunto de dados sintéticos com 10 características e 2 classes (-1 e +1)\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "# Separando o conjunto de dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "\n",
    "# Criando um modelo de classificação de Gradient Boosting \n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Definindo os hiperparâmetros a serem testados no GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'max_depth': [1,5]\n",
    "    }\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Realizando o GridSearch para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo os hiperparâmetros que tiveram o melhor resultado\n",
    "melhores_hparam = grid_search.best_params_\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(melhores_hparam )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5d6b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para cada fold:  [0.92   0.9375 0.9375 0.925  0.92  ]\n",
      "Média dos scores:  0.9279999999999999\n"
     ]
    }
   ],
   "source": [
    "#Criando arvore com melhores parâmetros e avaliando\n",
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Criando o classificador Gradient Boost com os melhores hiperparâmetros encontrados utilizando o grid_Search.best_params \n",
    "clf = GradientBoostingClassifier(**melhores_hparam) # o operador ** é utilizado para passar um dicionário como argumento para uma função ou construtor, descompactando-o em pares chave-valor.\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o clf\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print('Score para cada fold: ', scores)\n",
    "\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "print('Média dos scores: ' , scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c516002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para cada fold:  [0.92   0.9375 0.9375 0.925  0.92  ]\n",
      "Média dos scores:  0.9279999999999999\n"
     ]
    }
   ],
   "source": [
    "#Outra forma utilizando best_estimator\n",
    "#Recebe o classificador Gradiente Boost \"fitado\" com os dados de treino e com os melhores hiperparâmetros encontrados\n",
    "clf = grid_search.best_estimator_\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o Gradient Boost\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print('Score para cada fold: ', scores)\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "scores.mean()\n",
    "print('Média dos scores: ' , scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4834f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9279"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando o desempenho do modelo nos dados de teste\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afea4dc",
   "metadata": {},
   "source": [
    "**Regressão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cf0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 150}\n",
      "CPU times: total: 141 ms\n",
      "Wall time: 974 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error # Função para calcular o erro quadrático médio\n",
    "from sklearn.datasets import make_friedman1 # Função para gerar dados sintéticos do conjunto de Friedman\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Implementação do regressor de Gradient Boosting do scikit-learn\n",
    "\n",
    "# Gerando um conjunto de dados sintéticos do conjunto de Friedman com 1200 amostras e ruido adicional de 1.0\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "# Separando o conjunto de dados em conjuntos de treinamento e teste\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "# Criando um regressor de Gradient Boosting \n",
    "est = GradientBoostingRegressor(random_state=0,loss='squared_error')\n",
    "\n",
    "# Definindo os hiperparâmetros a serem testados no GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'max_depth': [1,5]\n",
    "    }\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=est, param_grid=param_grid, cv=5, n_jobs = -1)\n",
    "\n",
    "# Realizando o GridSearch para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtendo os hiperparâmetros que tiveram o melhor resultado\n",
    "melhores_hparam = grid_search.best_params_\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(melhores_hparam )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3687d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score para cada fold:  [0.73658007 0.74687921 0.77092803 0.75671773 0.80889851]\n",
      "Média dos scores:  0.7640007092913994\n"
     ]
    }
   ],
   "source": [
    "#Criando o regressor com melhores parâmetros e avaliando\n",
    "est = grid_search.best_estimator_\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o Gradient Boost\n",
    "scores = cross_val_score(est, X_train, y_train, cv=5)\n",
    "print('Score para cada fold: ', scores)\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "scores.mean()\n",
    "print('Média dos scores: ' , scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "312941e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838832232098804"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avaliando o desempenho do modelo nos dados de teste\n",
    "#clf.fit(X_train,y_train)\n",
    "est.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9bccc",
   "metadata": {},
   "source": [
    "**5) Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?**\n",
    "\n",
    "A maior diferença entre o GBM e o Stochastic GBM é a forma como as amostras são selecionadas para o ajuste das árvores de decisão. No Gradient Boosting tradicional (GBM), todas as amostras do conjunto de treinamento são usadas para ajustar cada árvore de decisão individual. Já no Stochastic GBM, a seleção das amostras é feita de forma estocástica (aleatória). Em vez de usar todas as amostras para ajustar cada árvore, uma fração (subconjunto) aleatória das amostras sem repetição é usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d8964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
