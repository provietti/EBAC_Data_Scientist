{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3afd16",
   "metadata": {},
   "source": [
    "# EBAC - Profissão Cientista de Dados\n",
    "## Módulo 23: Combinação de Modelos I\n",
    "## Exercício 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd6466",
   "metadata": {},
   "source": [
    "**1) Cite 5 diferenças entre o Random Forest e o\n",
    "AdaBoost**\n",
    "\n",
    "- Método de construção do modelo: O Random Forest é um algoritmo de bagging que combina várias árvores de decisão para tomar decisões finais, enquanto o Adaboost é um algoritmo de boosting que combina várias versões de um classificador fraco chamado stumps(toco em português) que é uma árvore de apenas 1 nó e duas folhas.\n",
    "\n",
    "\n",
    "- Independência entre as árvores: No Adaboost, para cada item da base de treinamento é atribuído a um peso que é atualizado a cada iteração do algoritmo. Itens mal classificados recebem pesos mais altos para que o próximo classificador fraco se concentre neles. No Random Forest, cada árvore é treinada independentemente, sem considerar pesos para os exemplos de treinamento.\n",
    "\n",
    "\n",
    "- Peso das respostas das árvores:  No Random Forest, a decisão final é tomada por meio de votação majoritária das árvores individuais. Cada árvore contribui com um voto igualmente ponderado. No Adaboost, a decisão final é baseada na soma ponderada dos resultados dos stumps, onde cada um deles tem um peso atribuído com base em seu desempenho.\n",
    "\n",
    "\n",
    "- Desbalanceamento de classes: O Random Forest é mais robusto a conjuntos de dados desbalanceados, onde as classes têm quantidades significativamente diferentes de instâncias/registros. Como as amostras são selecionadas aleatoriamente com substituição, as árvores podem ser treinadas com instâncias de diferentes classes evitando que as árvores sejam excessivamente influenciadas pela classe majoritária e permite que elas aprendam caracterípois quando uma classe tem uma quantidade significativamente menor de exemplos do que a outra, os exemplos da classe minoritária geralmente são considerados mais difíceis de classificar corretamente e os stumps irão errar mais a classificação destas instâncias, portanto elas vão receber pesos mais altos o que significa que os classificadores fracos são incentivados a se concentrar mais nesses exemplos durante o treinamento. Isso pode levar a uma classificação inadequada da classe majoritária, já que ela recebe menos atenção durante o treinamento.\n",
    "\n",
    "\n",
    "- Treinamento em paralelo: O Random Forest permite treinar as árvores individualmente em paralelo, o que pode acelerar o processo de treinamento em sistemas com recursos de processamento paralelo. Já o Adaboost treina os stumps sequencialmente, onde cada classificador depende dos resultados do anterior, tornando o treinamento mais suscetível a restrições de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181bdb0",
   "metadata": {},
   "source": [
    "**2) Acesse o link do Scikit-learn – adaboost, leia a explicação (traduza se for preciso) e crie um jupyter notebook contendo o exemplo do AdaBoost.**\n",
    "https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57997163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9466666666666665"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Carregando o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "#Cria um classificador AdaBoost com 100 estimadores (classificadores fracos -stumps)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o AdaBoost\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb6a82",
   "metadata": {},
   "source": [
    "**3) Cite 5 Hyperparametros importantes no AdaBoost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ca928c",
   "metadata": {},
   "source": [
    " **- base_estimator:** Este hiperparâmetro define o classificador fraco utilizado como base para construir o classificador forte no AdaBoost. Por default o base_estimator é uma árvire de decição com profundidade máxima igual a 1, ou seja, é um objeto criado pela função DecisionTreeClassifier(max_depth=1).\n",
    " \n",
    "\n",
    "**- n_estimators:** Esse hiperparâmetro define o número de estimadores (classificadores fracos -stumpers) que serão combinados para formar o classificador forte. Um maior número de estimadores pode aumentar a capacidade de aprendizado do modelo, mas também pode levar a um aumento no tempo de treinamento.\n",
    "\n",
    "\n",
    "**- learning_rate:** Esse hiperparâmetro define o peso aplicado a cada classificador em cada iteração de reforço. Uma maior learning_rate  aumenta a contribuição de cada classificador.\n",
    "\n",
    "\n",
    "**- algorithm:** Esse hiperparâmetro especifica o algoritmo utilizado para atualizar os pesos das instâncias durante o treinamento do AdaBoost. As opções são \"SAMME\" e \"SAMME.R\". O \"SAMME.R\" é uma versão otimizada do \"SAMME\" que permite estimar probabilidades de classe e que normalmente converge mais rápido do que o SAMME.\n",
    "\n",
    "\n",
    "**- random_state:** Esse hiperparâmetro controla a semente usada pelo gerador de números aleatórios. Isso afeta a reprodutibilidade dos resultados. Definir um valor fixo para random_state permite que os resultados sejam reproduzidos em diferentes execuções."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c9b51",
   "metadata": {},
   "source": [
    "**4) (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo (load_iris)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85eb5520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores hiperparâmetros encontrados:\n",
      "{'learning_rate': 0.5, 'n_estimators': 50}\n",
      "CPU times: total: 8.48 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Carregando o conjunto de dados Iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "#Cria um classificador AdaBoost com 100 estimadores (classificadores fracos -stumps)\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# Definindo os hiperparâmetros a serem testados no GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200,300],\n",
    "    'learning_rate': [0.1, 0.5, 1.0]\n",
    "    \n",
    "    }\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Realizando o GridSearch para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Obtendo os hiperparâmetros que tiveram o melhor resultado\n",
    "melhores_hparam = grid_search.best_params_\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores hiperparâmetros encontrados:\")\n",
    "print(melhores_hparam )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5d6b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Criando o classificador AdaBoost com os melhores hiperparâmetros encontrados utilizando o grid_Search.best_params \n",
    "clf = AdaBoostClassifier(**melhores_hparam) # o operador ** é utilizado para passar um dicionário como argumento para uma função ou construtor, descompactando-o em pares chave-valor.\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o AdaBoost\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c516002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Outra forma utilizando best_estimator\n",
    "#Recebe oo classificador AdaBoost com os melhores hiperparâmetros encontrados\n",
    "clf = grid_search.best_estimator_\n",
    "# Realiza a validação cruzada (k-fold) com 5 folds usando o AdaBoost\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "#Calcula a média das pontuações obtidas durante a validação cruzada\n",
    "scores.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
